---
title: "Model diagnostic"
author: "Xiaoyang Li"
date: "2019/12/8"
output: word_document
---
```{r include=FALSE}
library(tidyverse)
library(HH)
```


```{r include=FALSE}
data = read_csv('./data/Lawsuit.csv') %>%
  janitor::clean_names() %>%
  mutate(
    gender = factor(gender, levels = c(0,1), labels = c("female","male")),
    clin = factor(clin, levels = c(0,1)),
    cert = factor(cert, levels = c(0,1)),
    rank = factor(rank, levels = c(1,2,3))
  )

data$ln_sal94 = log(data$sal94)
data$ln_sal95 = log(data$sal95)
data$salavg = (data$sal94 + data$sal95)/2
data$ln_salavg = log(data$salavg)

reg13 = lm(ln_salavg ~ gender*exper + dept + clin + cert + rank, data = data)
summary(reg13)
```


# Model Diagnostic


## Functional form for continuous predictor
```{r scatter plot, echo=FALSE, message=FALSE}
data %>% 
  ggplot(aes(x = exper, y = ln_salavg)) +
  geom_point() +
  geom_smooth()
```

The only continuous variable is `exper`. So we made scatter plot for `exper` and `ln_salavg`. According to the scatter plot, we hypothesized there is non-linear relationship between `exper` and `ln_salavg`. 

### Polynomial regression


```{r polynomial without center exper, include=FALSE}
data_poly = data %>% 
  mutate(exper_pow2 = exper^2,
         exper_pow3 = exper^3,
         exper_pow4 = exper^4)

poly_exper = lm(ln_salavg ~ gender*exper + factor(dept) + clin + cert + rank + exper + exper_pow2 + exper_pow3 + exper_pow4 , data = data_poly) 

summary(poly_exper)

anova(reg13, poly_exper)

```


```{r polynomial with center exper, include=FALSE}
data_poly_ = data %>% 
  mutate(
    exper_ = exper - mean(exper),
    exper_pow2_ = exper_^2,
    exper_pow3_ = exper_^3,
    exper_pow4_ = exper_^4)


poly_exper_ = lm(ln_salavg ~ gender*exper + factor(dept) + clin + cert + rank + exper_pow2_ + exper_pow3_ + exper_pow4_, data = data_poly_) 

summary(poly_exper_)

anova(reg13, poly_exper_)
```

We added its power 2, 3, 4 to our model and its centering power 2, 3, 4 seperately to create polynomial models. But the summary result indicates that there is no association between higher order of `exper` and `ln_salavg`. Alao, by using anova, we fail to reject our reg13. There is no need to use high order model to improve our `reg13`.


### Piecewise model

To make piecewise model, I choose exper = 7.5 and exper = 9 as knots according to the scatter plot.

```{r piecewise,include=FALSE}
data_piecewise = data %>% 
  mutate(spline_7.5 = (exper - 7.5) * (exper >= 7.5),
         spline_9 = (exper - 9) * (exper >= 9))

pw_exper = lm(ln_salavg ~ gender*exper + factor(dept) + clin + cert + spline_7.5 + spline_9 + rank, data = data_piecewise)

summary(pw_exper)

anova(reg13, pw_exper)
```
P-value of 2 splines is larger than 0.05 in result of piecewise model. Alao, Alao, by using anova, we fail to reject our reg13. So there is no need to use piecewise model to improve our `reg13`.


## Diagnostic

### Multicollinearity
```{r include = FALSE}
## stratify by exper
data_stra1 = filter(data, exper < 10)
data_stra2 = filter(data, exper >= 10 & exper < 20)
data_stra3 = filter(data, exper >= 20)
reg_stra1 = lm(ln_salavg ~ gender + factor(dept) + clin + cert + rank, data = data_stra1)
summary(reg_stra1)
reg_stra2 = lm(ln_salavg ~ gender + factor(dept) + clin + cert + rank, data = data_stra2)
summary(reg_stra2)
reg_stra3 = lm(ln_salavg ~ gender + factor(dept) + clin + cert + rank, data = data_stra3)
summary(reg_stra3)
```


```{r echo=FALSE}
vif(reg_stra1) %>% knitr::kable()
vif(reg_stra2) %>% knitr::kable()
vif(reg_stra3) %>% knitr::kable()
```

After stratifying for experience, we tested VIF for 3 layers of experience. According to the vif results above, there is no predictors' vif larger than 5. So none of predictor's coefficient might be misleading due to collinearity.

### Assumptions
```{r echo = FALSE}
par(mfrow = c(2,2))
plot(reg13)
```

Based on the diagnostic plots above, we can see the constant variance assumptions hold well. The curves in residuals vs fitted plot are horizontal and bounce around 0 which indicates that constant variance is satisfied in our model. Scale-location plot shows the same thing. No.184 observations deviates from the normal line on the qq plot means it might be a outlier. Excepr from this observation, other observations satisfy normality pretty well.

In addition, the same observation is close to the 0.5 cook's distance in residuals vs leverage plot, which means No.184 observation might be influential.

### Outlier and influential points

According to residuals vs fitted plot and scale-location plot, we can find that No.82, No.122, No. 184 observation are outliers. The No.184 obervation are far away from others.


```{r include=FALSE}
data_o = data[-c(184),]
reg13_o = lm(ln_salavg ~ gender*exper + factor(dept) + clin + cert + rank, data = data_o)
summary(reg13_o)
```
After deleting no.184 observation, adjusted R-square improve from 0.9337 to 0.9458. In addition, several coefficient change. Specifically, coefficient of gender changes a lot (0.129 to 0.098). 

```{r include=FALSE}
data_o_ = data_o[-c(82, 122),]
reg13_o_ = lm(ln_salavg ~ gender*exper + factor(dept) + clin + cert + rank, data = data_o_)
summary(reg13_o_)
```

Then we deleted other outliers(No. 82, No. 122), adjusted R-square improve didn't improve very much. BBut, coefficient of gender changes a lot (0.098 to 0.084).

To summary, outliers containning  No.82, No.122 and No. 184 observation. They are influential points in the same time, which means there might be other potential preditor we didn't take into consideration in our model.


```{r include=FALSE}
influence.measures(reg13) # 19,44,56,58,59,88,125,135,172,216,239
```

According to influence mesure result, we found that No. 19, No. 44, No. 56, No. 58, No. 59, No. 88, No. 122, No. 125, No. 135, No. 172, No. 184, No. 216, No. 239 are potential influential points. Except from outliers we deleted above, we delete other potential influential points in `reg_i`.

```{r include=FALSE}
data_i = data_o_[-c(19,44,56,58,59,88,125,135,172,216,239),]
reg13_i = lm(ln_salavg ~ gender*exper + factor(dept) + clin + cert + rank, data = data_i)
summary(reg13_i)
```

After comparing summary results of `reg13_o_` and `reg13_i`, adjusted R-squared improve a little. Coefficients of all predictor don't change largely. So none of our potential influential observation are really influential points after testing. 


