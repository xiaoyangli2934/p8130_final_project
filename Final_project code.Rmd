---
title: "Final project code"
author: "Xiaoyang Li(xl2934) & Guangling Xu(gx2144) & Shenglin Liu(sl4659) & Jingyu Fu(jf3286)"
date: "2019/12/16"
output: html_document
---
```{r set up, include = FALSE}
library(arsenal)
library(HH)
library(olsrr)
library(broom)
library(formattable)
library(htmltools)
library(webshot)
library(patchwork)
library(MASS)
library(tidyverse)

knitr::opts_chunk$set(
    echo = TRUE,
    warning = FALSE,
    fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme(panel.grid =element_blank()) 
theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Data Exploration

```{r data import, include=FALSE}
data_raw = data = read_csv('./data/Lawsuit.csv') %>%
  janitor::clean_names()

data = read_csv('./data/Lawsuit.csv') %>%
  janitor::clean_names() %>%
  mutate(
    dept = factor(dept, levels = c(1,2,3,4,5,6), labels = c("Biochemistry/Molecular Biology","Physiology","Genetics","Pediatrics","Medicine","Surgery")),
    gender = factor(gender, levels = c(0,1), labels = c("Female","Male")),
    clin = factor(clin, levels = c(0,1), labels = c("Primarily research emphasis","Primarily clinical emphasis")),
    cert = factor(cert, levels = c(0,1), labels = c("not certified","Board certified")),
    rank = factor(rank, levels = c(1,2,3), labels = c("Assistant","Associate","Full"))
  )

data$ln_sal94 = log(data$sal94)
data$ln_sal95 = log(data$sal95)
data$salavg = (data$sal94 + data$sal95)/2
data$ln_salavg = log(data$salavg)
```

### Data Description

**Table 1: Summary Statistics of lawsuit Data**
```{r, results="asis", echo=FALSE}
my_controls <- tableby.control(
  test = T,
  total = T,
  numeric.test = "kwt", cat.test = "chisq",
  numeric.stats = c("meansd", "medianq1q3", "range", "Nmiss2"),
  cat.stats = c("countpct", "Nmiss2"),
  stats.labels = list(
    meansd = "Mean (SD)",
    medianq1q3 = "Median (Q1, Q3)",
    range = "Min - Max",
    Nmiss2 = "Missing"
  )
)
 
my_labels <- list(
  dept = "Department",
  clin = "Clinical Emphasis",
  cert = "Certification",
  prate = "Publication Rate",
  exper = "Experience",
  rank = "Rank",
  sal94 = "Salary in 1994",
  sal95 = "Salary after Increment in 1995"
)

data1 = data %>% 
  dplyr::select(-id)

table_two <- tableby(gender ~ .,
  data = data1 ,
  control = my_controls
)
 
summary(table_two,
  labelTranslations = my_labels,
  pfootnote = TRUE,text = FALSE
) 
```

* Values are count(percentage) for categorical variables or mean(SD), median(25% quantile, 75% quantile), min-max, count of missing values for continuous variables.* Publication Rate: Number of years between CV date and MD date;Experience: Number of years since obtaining MD; Rank: a proxy for productivity

Among the 261 participants in this study, 40.6% (n = 106) were female. As shown in *Table 1, Graph 1*, participants who were female were more likely to be in the department of medicine , premarily clinical emphasis, board certified and assistant. Participants who were male were more likely to be in the department of medicine, premarily clinical emphasis, board certified, full professor.The mean publication rate was 4.6(sd = 1.9) for male and 5.4(sd = 1.9) for female. The mean number of years since obtaining MD was 12.1(sd = 6.7)for male and 7.5(sd = 4.2) for female. The mean salary in 1994 was 177338.8(sd = 85930.5) for male and 118871.3(sd = 56168.0) for female. Salary after increment in 1995 was 194914.1(sd = 94902.7) for male and 130876.9(sd = 88778.4) for female. 

### Data Distribution

In *Graph 1*, distributions of publication rate among male and female are alike.The publication rate of male concentrated around 3 pieces per year and 7.4 pieces per year,The publication rate of female concentrated around 3.7 pieces per year and 7.6 per year.Female published faster than male on average. Plot of distribution of number of years since obtaining MD showed that female, on average, had less experiences than male.The distributions of two genders were all right-skewed, meaning there were some extreme high values of experiences. Distribution plot of salary in 1994 indicated that female earned less than male on average.Some extreme high values of salary biased the distribution into right-skewed. Distribution plot of salary in 1995 after increment illustrated that female earned less than male on average.Some extreme high values of salary biased the distribution into right-skewed. Overall, salary distribution in 1994 and 1995 looks alike.Therefore, mean of these two years were used in our model as the response.

**Graph 1: Distribution of Categorical predictors and Continuous predictors in lawsuit dataset.Grouped by gender**
```{r,echo = FALSE,fig.width = 8, fig.height = 10}
lawsuit = data %>% 
  group_by(gender) %>% 
  mutate(
    mean_prate = mean(prate),
    mean_exper = mean(exper),
    mean_sal94 = mean(sal94),
    mean_sal95 = mean(sal95),
    mean_year = mean(sal94 + sal95)/2
  )

dept_plot = ggplot(lawsuit, aes(x = dept)) +
  geom_bar(aes(fill = gender))+
   theme(axis.text.x = element_text(vjust = 0.5, hjust = 0.5, angle = 45))+ theme(legend.position = "top")+
  labs(
    title = "Distribution of Department by Gender",
    x = "Department",
    y = "Count"
  )
           
clin_plot = ggplot(lawsuit, aes(x = clin))+
  geom_bar(aes(fill = gender))+
  theme(axis.text.x = element_text(vjust = 0.5, hjust = 0.5, angle = 45))+ theme(legend.position = "top")+
  labs(
    title = "Distribution of Primarily Emphasis by Gender",
    x = "Primarily Emphasis",
    y = "Count"
  )

cert_plot = ggplot(lawsuit, aes(x = cert))+
  geom_bar(aes(fill = gender))+
  theme(axis.text.x = element_text(vjust = 0.5, hjust = 0.5, angle = 45))+ theme(legend.position = "top")+
  labs(
    title = "Distribution of Certification Status by Gender",
    x = "Certification Status",
    y = "Count"
  )

prate_plot = ggplot(lawsuit, aes(x = prate))+
  geom_density(aes(fill= gender,y = ..count..),alpha = 0.4)+
   geom_vline(aes(xintercept = mean_prate, color = gender),
             data = lawsuit, linetype = "dashed")+
  labs(
    title = "Distribution Publication Rate by Gender",
    x = "Publication Rate",
    y = "Count"
  )


rank_plot = ggplot(lawsuit, aes(x = rank))+
  geom_bar(aes(fill = gender))+
  theme(axis.text.x = element_text(vjust = 0.5, hjust = 0.5, angle = 45))+ theme(legend.position = "top")+
  labs(
    title = "Distribution of Rank by Gender",
    x = "Rank",
    y = "Count"
  )

exper_plot = ggplot(lawsuit, aes(x = exper))+
  geom_density(aes(fill= gender,y = ..count..),alpha = 0.4)+
   geom_vline(aes(xintercept = mean_exper, color = gender),
             data = lawsuit, linetype = "dashed")+
  labs(
    title = "Distribution of Experience by Gender",
    x = "Experience",
    y = "Count"
  )

sal94_plot = ggplot(lawsuit, aes(x = sal94))+
  geom_density(aes(fill = gender, y = ..count..), alpha = 0.4)+
  geom_vline(aes(xintercept = mean_sal94, color = gender),
             data = lawsuit, linetype = "dashed")+
  labs(
    title = "Distribution of Salary in 1994 by Gender",
    x = "Salary in 1994",
    y = "Count"
  )

sal95_plot = ggplot(lawsuit, aes(x = sal95))+
  geom_density(aes(fill = gender, y = ..count..), alpha = 0.4)+
  geom_vline(aes(xintercept = mean_sal95, color = gender),
             data = lawsuit, linetype = "dashed")+
   labs(
    title = "Distribution of Salary in 1995 by Gender",
    x = "Salary in 1995",
    y = "Count"
  )

mean_plot = ggplot(lawsuit, aes(x = sal95))+
  geom_density(aes(fill = gender, y = ..count..), alpha = 0.4)+
  geom_vline(aes(xintercept = mean_year, color = gender),
             data = lawsuit, linetype = "dashed")+
   labs(
    title = "Distribution of Mean Salary in two years",
    x = "Mean Salary in two years",
    y = "Count"
  ) 

(dept_plot | clin_plot)/
(cert_plot | rank_plot)
  
(prate_plot |exper_plot)/(
(sal94_plot |sal95_plot)/mean_plot)
```

### Transformation

Based on the investigation of the distribution, to meet the assumption that response value y is normal, "salary" needs transformation. 

```{r, echo = FALSE, fig.height=10/2.54, fig.width= 10/2.54}

data$ln_sal94 = log(data$sal94)
data$ln_sal95 = log(data$sal95)
data$salavg = (data$sal94 + data$sal95)/2


multi_fit = lm(salavg ~ factor(dept)+ factor(gender)+ factor(clin)+ factor(cert)+ prate + exper +factor(rank) , data = data)

boxcox(multi_fit)

data$ln_salavg = log(data$salavg)
```
Based on the plot above, we need to do log-transformation for the mean value of sal94 and sal95 to meet the assumption of normality.

## Model Regression

```{r multicollinearity}
# all covariates
reg1 = lm(ln_salavg ~ gender + dept + clin + prate + cert + exper + rank, data = data)
summary(reg1)
vif(reg1) %>% 
  knitr::kable()

pred = data_raw[,c(2, 4:8)]
round(cor(pred), 3) %>% 
  broom::tidy() %>% 
  knitr::kable()

# all covariates except clin
reg2 = lm(ln_salavg ~ gender + dept + prate + cert + exper + rank, data = data)
vif(reg2) %>% 
  knitr::kable()
# all covariates except prate
reg3 = lm(ln_salavg ~ gender + dept + clin + cert + exper + rank, data = data)
vif(reg3) %>% 
  knitr::kable()
# dropping prate leads to lower mean vif
```


```{r confounding}
# gender only
reg4 = lm(ln_salavg ~ gender, data = data)
summary(reg4)
# gender + dept
reg5 = lm(ln_salavg ~ gender + dept, data = data)
summary(reg5)
# gender + clin
reg6 = lm(ln_salavg ~ gender + clin, data = data)
summary(reg6)
# gender + cert
reg7 = lm(ln_salavg ~ gender + cert, data = data)
summary(reg7)
# gender + exper
reg8 = lm(ln_salavg ~ gender + exper, data = data)
summary(reg8)
# gender + rank
reg9 = lm(ln_salavg ~ gender + rank, data = data)
summary(reg9)
# all confounders
summary(reg3)
```


```{r interaction}
# 3 + gender * dept
reg10 = lm(ln_salavg ~ gender*dept + clin + cert + exper + rank, data = data)
summary(reg10)
# 3 + gender * clin
reg11 = lm(ln_salavg ~ gender*clin + dept + cert + exper + rank, data = data)
summary(reg11)
# 3 + gender * cert
reg12 = lm(ln_salavg ~ gender*cert + dept + clin + exper + rank, data = data)
summary(reg12)
# 3 + gender * exper
reg13 = lm(ln_salavg ~ gender*exper + dept + clin + cert + rank, data = data)
summary(reg13)
# 3 + gender * rank
reg14 = lm(ln_salavg ~ gender*rank + dept + clin + cert + exper, data = data)
summary(reg14)
# both exper and rank are modifiers
# 3 + gender * rank + gender * exper
reg15 = lm(ln_salavg ~ gender*rank + gender*exper + dept + clin + cert, data = data)
summary(reg15)
# gender * rank becomes non-significant
# reg15 is not superior compared to reg13
reg1315 = anova(reg13, reg15)

# reg15 is superior compared to 14
reg1415 = anova(reg14, reg15)

# make table for the result for anova
nested_anova = data.frame(
  Fstatistics = c(reg1315$F, reg1415$F),
  P.value = c(reg1315$`Pr(>F)`, reg1415$`Pr(>F)`)
) %>% 
  drop_na() 

nested_anova$model = c("reg13 VS reg15", "reg14 VS reg15")
nested_anova = nested_anova %>% 
  dplyr::select(model, everything())
nested_anova %>% 
  knitr::kable()

# reg13 reg14 is similar
summary_reg13 = summary(reg13)
summary_reg14 = summary(reg14)
# make table for the result
anova_table = 
data.frame(adj.R2 = c(summary_reg13$adj.r.squared,summary_reg14$adj.r.squared),
Fstatistics = c(summary_reg13$fstatistic,summary_reg14$fstatistic)
)
anova_table1 = data.frame(anova_table[c(1,4),])
anova_table1$F.test = c(1.79, 1.76)
anova_table1$model = c("model1","model2")
anova_table1 = anova_table1 %>% 
  dplyr::select(model, adj.R2, Fstatistics, F.test) %>%   knitr::kable()
anova_table1

```


```{r selection}
# compare with and without interaction term
anova(reg3, reg13)

# AIC
AIC(reg13)
AIC(reg14)

# BIC 
AIC(reg13, k = log(length(data$ln_salavg)))
AIC(reg14, k = log(length(data$ln_salavg)))
```

```{r stratification}
# stratification by exper
data_low = filter(data, exper < 10)
data_medium = filter(data, exper >= 10 & exper < 20)
data_high = filter(data, exper >= 20)
reg_low = lm(ln_salavg ~ gender + dept + clin + cert + rank, data = data_low)
summary(reg_low)
reg_medium = lm(ln_salavg ~ gender + dept + clin + cert + rank, data = data_medium)
summary(reg_medium)
reg_high = lm(ln_salavg ~ gender + dept + clin + cert + rank, data = data_high)
summary(reg_high)

## stratify by rank"Assistant","Associate","Full"
data_stra1_ = filter(data, rank == "Assistant")
data_stra2_ = filter(data, rank == "Associate")
data_stra3_ = filter(data, rank == "Full")
reg_stra1_ = lm(ln_salavg ~ gender + dept + clin + cert + exper, data = data_stra1_)
summary(reg_stra1_)
reg_stra2_ = lm(ln_salavg ~ gender + dept + clin + cert + exper, data = data_stra2_)
summary(reg_stra2_)
reg_stra3_ = lm(ln_salavg ~ gender + dept + clin + cert + exper, data = data_stra3_)
summary(reg_stra3_)
```

```{r tabular}
# function to export formattable
export_formattable = function(f, file, width = "100%", height = NULL, background = "white", delay = 0.2)
    {w = as.htmlwidget(f, width = width, height = height)
     path = html_print(w, background = background, viewer = NULL)
     url = paste0("file:///", gsub("\\\\", "/", normalizePath(path)))
     webshot(url,
             file = file,
             selector = ".formattable_widget",
             delay = delay)
     }
# confounding
tidy4 = tidy(reg4)
tidy5 = tidy(reg5)
tidy6 = tidy(reg6)
tidy7 = tidy(reg7)
tidy8 = tidy(reg8)
tidy9 = tidy(reg9)
con_df = rbind(tidy4[2, c(2:5)], tidy5[2, c(2:5)], tidy6[2, c(2:5)], tidy7[2, c(2:5)], tidy8[2, c(2:5)], tidy9[2, c(2:5)])
con_df$coefficient.change = abs((con_df$estimate-tidy4$estimate[2])/tidy4$estimate[2])
con_df$confounder = con_df$coefficient.change
con_df$covariate = c("gender", "dept", "clin", "cert", "exper", "rank")
con_ft = formattable(con_df[, c(7,1,4,5,6)], 
                     align = c("l", rep("c", 4)),
                     list(`covariate` = formatter("span", style = ~ style(color = "grey", font.weight = "bold")), `coefficient.change` = percent, `confounder` = formatter("span", x ~ icontext(ifelse(x > 0.1, "ok", "remove"), ifelse(x > 0.1, "Yes", "No")), style = x ~ style(color = ifelse(x > 0.1, "green", "red"))))
                     )
# export formattable con_ft
export_formattable(con_ft,"con_ft.png")
# interaction
tidy10 = tidy(reg10)
tidy11 = tidy(reg11)
tidy12 = tidy(reg12)
tidy13 = tidy(reg13)
tidy14 = tidy(reg14)
int_df = rbind(tidy10[c(13:17), c(1:5)], tidy11[13, c(1:5)], tidy12[13, c(1:5)], tidy13[13, c(1:5)], tidy14[c(13:14), c(1:5)])
int_df$interaction = int_df$p.value
names(int_df)[1] = "covariate"
int_ft = formattable(int_df[, c(1,2,5,6)], 
                     align = c("l", rep("c", 3)),
                     list(`covariate` = formatter("span", style = ~ style(color = "grey", font.weight = "bold")), `interaction` = formatter("span", x ~ icontext(ifelse(x < 0.05, "ok", "remove"), ifelse(x < 0.05, "Yes", "No")), style = x ~ style(color = ifelse(x < 0.05, "green", "red"))))
                     )
# export formattable int_ft
export_formattable(int_ft,"int_ft.png")
```

```{r result, message = FALSE}
result_df = rbind(tidy4[2, c(2:5)], tidy13[2, c(2:5)], tidy14[2, c(2:5)])
result_df$model = c("unadjusted", "model1", "model2")
result_ft = formattable(result_df[, c(5,1,2,3,4)], 
                     align = c("l", rep("c", 4)),
                     list(`model` = formatter("span", style = ~ style(color = "grey", font.weight = "bold")))
                     )
# export formattable result_ft
export_formattable(result_ft,"result_ft.png")
```

## Model Diagnostic


### Functional form for continuous predictor
```{r scatter plot, echo=FALSE, message=FALSE}
non_linear = data %>% 
  ggplot(aes(x = exper, y = ln_salavg)) +
  geom_point() +
  geom_smooth() +
  labs(
    x = "Experience(years)",
    y = "ln(average of salary in 94 and 95)"
  )

```

The only continuous variable in our model is `exper`. So we made scatter plot for `exper` and `ln_salavg`. According to the scatter plot, we hypothesized there is non-linear relationship between `exper` and `ln_salavg`. 

#### Polynomial regression

For model 1 (interaction gender*exper)

```{r polynomial without center exper for 1}
data_poly = data %>% 
  mutate(exper_pow2 = exper^2,
         exper_pow3 = exper^3,
         exper_pow4 = exper^4)

poly_exper1 = lm(ln_salavg ~ gender*exper + dept + clin + cert + rank + exper + exper_pow2 + exper_pow3 + exper_pow4 , data = data_poly) 

summary(poly_exper1)

anova(reg13, poly_exper1)

```


```{r polynomial with center exper for 1}
data_poly_ = data %>% 
  mutate(
    exper_ = exper - mean(exper),
    exper_pow2_ = exper_^2,
    exper_pow3_ = exper_^3,
    exper_pow4_ = exper_^4)


poly_exper_1 = lm(ln_salavg ~ gender*exper + dept + clin + cert + rank + exper_pow2_ + exper_pow3_ + exper_pow4_, data = data_poly_) 

summary(poly_exper_1)

anova(reg13, poly_exper_1)
```

We added its centering power 2, 3, 4 to original model 1. But the summary result indicates that there is no association between higher order of `exper` and `ln_salavg`. Alao, we fail to reject original model 1 by using anova to compare original model with polinomial model.


For model 2 (interaction gender*rank)

```{r polynomial without center exper for 2}
poly_exper2 = lm(ln_salavg ~ gender*rank + dept + clin + cert + rank + exper + exper_pow2 + exper_pow3 + exper_pow4 , data = data_poly) 

summary(poly_exper2)

anova(reg14, poly_exper2)

```


```{r polynomial with center exper for 2}
poly_exper_2 = lm(ln_salavg ~ gender*rank + dept + clin + cert + rank + exper + exper_pow2_ + exper_pow3_ + exper_pow4_, data = data_poly_) 

summary(poly_exper_2)

anova(reg14, poly_exper_2)
```

We use the same process to test whether polynomial regression could improve model 2. The higher order model doesn't perform better than original one.



#### Piecewise model

To make piecewise model, I choose exper = 7.5 and exper = 9 as knots according to the scatter plot.

```{r piecewise for 1}
data_piecewise = data %>% 
  mutate(spline_7.5 = (exper - 7.5) * (exper >= 7.5),
         spline_9 = (exper - 9) * (exper >= 9))

pw_exper1 = lm(ln_salavg ~ gender*exper + dept + clin + cert + spline_7.5 + spline_9 + rank, data = data_piecewise)

summary(pw_exper1)

anova(reg13, pw_exper1)
```

2 spline of piecewise model 1 are not significant, so there is not enough significant evidence to show piecewise form of model 1 is reasonable.

```{r piecewise for 2}
pw_exper2 = lm(ln_salavg ~ gender*rank + dept + clin + cert  + spline_9 + exper, data = data_piecewise)

summary(pw_exper2)

anova(reg14, pw_exper2)
```

For model 2, P-value of spline at exper = 9 is 0.001 which indicate coefficient of exper change significantly here. However, the adjusted R-square is far less than original model 2. Therefore, our original model 2 fits the data better.


### Multicollinearity

For model 1 (interaction gender*exper)

```{r}
## stratify by exper
data_stra1 = filter(data, exper < 10)
data_stra2 = filter(data, exper >= 10 & exper < 20)
data_stra3 = filter(data, exper >= 20)
reg_stra1 = lm(ln_salavg ~ gender + dept + clin + cert + rank, data = data_stra1)
summary(reg_stra1)
reg_stra2 = lm(ln_salavg ~ gender + dept + clin + cert + rank, data = data_stra2)
summary(reg_stra2)
reg_stra3 = lm(ln_salavg ~ gender + dept + clin + cert + rank, data = data_stra3)
summary(reg_stra3)

vif(reg_stra1) %>% knitr::kable()
vif(reg_stra2) %>% knitr::kable()
vif(reg_stra3) %>% knitr::kable()
```


For model 1, we tested VIF after stratification by experience and found there is no predictors' vif larger than 5. So none of predictor's coefficient might be misleading due to collinearity.

For model 2 (interaction gender*rank)

```{r}
## stratify by rank
data_stra1_ = filter(data, rank == "Assistant")
data_stra2_ = filter(data, rank == "Associate")
data_stra3_ = filter(data, rank == "Full")
reg_stra1_ = lm(ln_salavg ~ gender + dept + clin + cert + exper, data = data_stra1_)
summary(reg_stra1_)
reg_stra2_ = lm(ln_salavg ~ gender + dept + clin + cert + exper, data = data_stra2_)
summary(reg_stra2_)
reg_stra3_ = lm(ln_salavg ~ gender + dept + clin + cert + exper, data = data_stra3_)
summary(reg_stra3_)

vif(reg_stra1_) %>% knitr::kable()
vif(reg_stra2_) %>% knitr::kable()
vif(reg_stra3_) %>% knitr::kable()
```

Also, we tested VIF after stratification by rank for model 2, there is no collinearity between predicor of model 2.


### Assumptions

For model 1 (interaction gender*exper)
```{r }
par(mfrow = c(2,2))
plot(reg13)
```

Based on the diagnostic plots for model 1, we can see the constant variance assumptions hold well. The curves in residuals vs fitted plot are horizontal and bounce around 0 which indicates that constant variance is satisfied in our model. Scale-location plot shows the same thing. No.184 observations deviates from the normal line on the qq plot means it might be a outlier. Except from this observation, other observations satisfy normality pretty well.

In addition, the same observation is close to the 0.5 cook's distance in residuals vs leverage plot, which means No.184 observation might be influential.

For model 2 (interaction gender*rank)

```{r}
par(mfrow = c(2,2))
plot(reg14)
```

Diagnostic plots for model 2 show similar things, except from No.184 observation, other observations satisfy all of our assumptions very well. It might be influential point in the same time.


## Outlier and influential points


For model 1 (interaction gender*exper)

According to residuals vs fitted plot and scale-location plot, we can find that No.56, No.122, No. 184 observation are outliers. The No.184 obervation are far away from others.

```{r}
judge = function(x){
  data_o = filter(data, id != x)
  reg13_o = lm(ln_salavg ~ gender*exper + dept + clin + cert + rank, data = data_o)
  sum(abs((reg13_o$coefficients - reg13$coefficients) / reg13$coefficients) > 0.1)
}
judge(56)
judge(122)
judge(184)


data_o = data %>% 
  filter(id != 184)

reg13_o = lm(ln_salavg ~ gender*exper + dept + clin + cert + rank, data = data_o)
summary(reg13_o)
```

After deleting no.184 observation, adjusted R-square of model 1 improve from 0.9337 to 0.9458. In addition, several coefficient change. Specifically, coefficient of gender changes a lot (0.129 to 0.098). Then we deleted other outliers(No. 56, No. 122) seperately, adjusted R-square didn't improve. coefficient of gender changes from 0.098 to 0.123.

To summary, outliers containning  No.56, No.122 and No. 184 observation, only No.184 is influential points in the same time, which means there might be other potential preditor we didn't take into consideration in our model 1.


```{r}
influential1 = influence.measures(reg13)$infmat %>%
  as.data.frame() %>%
  dplyr::select(cook.d) %>%
  mutate(case = c(1:261)) %>%
  dplyr::select(case, everything()) %>% 
  filter(cook.d > 4 / 261)


```

```{r}
influential1 %>% knitr::kable()
```

According to cook's distance of every observation, we found that No. 56, No. 58, No. 59, No. 73, No. 82, No. 122, No. 135, No. 182, No. 184, No. 216, No. 220 are potential influential points (criterion Di > 4/n). 

```{r}
nrow(influential1)
output = vector(length = 11)

for (i in 1:11) {
  l = influential1[i,1]
  output[i] = judge(l)
}

cbind(case = influential1[,1], output) %>% 
  knitr::kable()

```

Among those potential influential points, only deleting No. 184 or No. 216 cause huge change to coefficient in model1 (criterion>10%). So No. 184 and No. 216 are influential points.

For model 2 (interaction gender*rank)

According to residuals vs fitted plot and scale-location plot, we can find that No. 122, No. 184, No.208 observation are outliers. The No.184 obervation are far away from others.
```{r}
judge_ = function(x){
  data_o = filter(data, id != x)
  reg14_o = lm(ln_salavg ~ gender*rank + dept + clin + cert + exper, data = data_o)
  sum(abs((reg14_o$coefficients - reg14$coefficients) / reg14$coefficients) > 0.1)
}

judge_(122)
judge_(184)
judge_(208)
```

After deleting no.184 observation, adjusted R-square improve from 0.9322 to 0.9445. In addition, several coefficient change significantly. Specifically, coefficient of gender changes a lot (0.074 to 0.046). Then we deleted other outliers(No. 122, No. 208) seperately, coefficients of predictor don't changes a lot.

To summary, outliers containning  No.122, No.184 and No. 208 observation. Only No. 184 observation is influential points in the same time, which means there might be other potential preditor we didn't take into consideration in our model 2.


```{r}
influential2 = influence.measures(reg14)$infmat %>%
  as.data.frame() %>%
  dplyr::select(cook.d) %>%
  mutate(case = c(1:261)) %>%
  dplyr::select(case, everything()) %>% 
  filter(cook.d > 4 / 261)
influential2[,1]
```

```{r}
influential2 %>% knitr::kable()
```

According to influence mesure result, we found that No. 56, No. 58, No. 59, No. 73, No. 82, No. 122, No. 182, No. 184, No. 208 are potential influential points. Except from outliers we deleted above, we delete other potential influential points.

```{r}
nrow(influential2)
output_ = vector(length = 8)

for (i in 1:8) {
  l = influential2[i,1]
  output_[i] = judge_(l)
}

cbind(case = influential2[,1], output) %>% 
  knitr::kable()
```
Among those potential influential points, only deleting No. 184 causes huge change to coefficient in model1 (criterion>10%). So No. 184 are influential points.

